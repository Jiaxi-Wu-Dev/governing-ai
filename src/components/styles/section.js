  import React from 'react';

  
  function MySection() {
      return(
          <React.Fragment>
              <div class = "container" id="myContainer">
                <div class="child1">
                        <h1 class ="title">Nondiscrimination and Equality</h1>
                        
                        <p>
                        As mentioned above, bias and discrimination have become central topics for those
concerned with the governance and social impact of AI systems.20 A number of high
profile studies have demonstrated that, as in the case of detecting skin color, certain

AI systems are inherently discriminatory. Alarming reports have detailed how discrim-
inatory algorithms are already deployed in the justice system, wherein judges use

these tools for sentencing that purport to predict the likelihood a criminal defendant
will reoffend.21
In Automating Inequality, Virginia Eubanks details how government actors implement
automated and surveillance technologies that harm marginalized groups.22 Eubanks
studied automated systems in the US that discriminated against the poor’s receipt of

DATA & SOCIETY 11

GOVERNING ARTIFICIAL INTELLIGENCE

government assistance. Automating Inequality includes a discussion of the Allegheny
Family Screening Tool (AFST), a predictive risk model deployed by the County Office
of Children, Youth, and Families to forecast child abuse and neglect. While the AFST
is only one step in a process that includes human decision-makers, Eubanks argues
that it makes workers in the agency question their own judgment and “is already
subtly changing how some intake screeners do their jobs.” Moreover, the system can
override its human co-workers to automatically trigger investigations into reports. The
model has inherent flaws: it only contains information about families who use public
services, making it more effective at targeting poor residents.23 Such discriminatory
effects can lead to harms in other human rights areas, such as education, housing,
family, and work.
Some governments are already using algorithmic systems to classify people based on

problematic categories. For example, there are reports that the government of Chi-
na is deploying systems to categorize people by social characteristics.24 This Social

Credit System is being developed to collect data on Chinese citizens and score them
according to their social trustworthiness, as defined by the government. The system
has punitive functions, such as shaming “debtors” by displaying their faces on large
screens in public spaces or blacklisting them from booking trains or flights.25
Historically, we have seen how governments’ use of national systems of social sorting

along predetermined physical categories can lead to discrimination against margin-
alized groups. In South Africa, a classification system built on databases that sorted

citizens by pseudoscientific racial taxonomies was deployed to implement the racist
and violent policies of the apartheid regime. This well-documented case serves as an

important cautionary tale for any widespread deployment of AI social scoring sys-
tems.26 Without safeguards, even AI systems built for mundane bureaucratic functions

can be repurposed to enact discriminatory policies of control.

The importance of equality and nondiscrimination has filtered down through the ratifi-
cation of treaties to provide the basis for post-war constitutions, state law, and judicial

interpretation.27 For example, the South African constitution, adopted in 1996, directly
accounted for the discriminatory policies of the past. The constitution establishes
equality, human dignity, and human rights as its legal foundations and core values.
There have been attempts to frame discrimination in machine learning algorithms as
a human rights issue, as in a recent World Economic Forum (WEF) report that raised
both concerns and possible solutions for biased decision-making.28 The report calls
for human rights to move to the center of AI discussions:
                        </p>
                        <p>
                        The report challenges companies to prioritize compliance with human rights stan-
dards and to perform rights-based due diligence. Among the recommendations is a

call for companies to actively include a diversity of input and norms in systems design.
Companies are also encouraged to provide mechanisms of access and redress that
make developers responsible for discriminatory outputs.

In May 2018, Amnesty International and Access Now led the drafting of The Toron-
to Declaration: Protecting the Rights to Equality and Non-Discrimination in Machine

Learning Systems.

30 The document grounds the current attention on AI bias in binding
international legal principles. The Toronto Declaration outlines the responsibilities
of both states and private sector actors in respect to the use of machine learning
systems, including mitigating discriminatory effects, transparency, and provision of

effective remedies to those harmed. It remains to be seen how influential the declara-
tion will become, as organizers are currently in the process of seeking endorsements,

particularly from AI companies. Even so, it represents a significant effort to translate
fundamental human rights for the AI space.
                        </p>
                </div>
                <div class="child2">

                </div>
              </div>
             
          </React.Fragment>
      )
  }
  
  export default MySection;